# BrainStorm 2026 - Track 2: The "Compass" Challenge

Build a real-time visualization tool to guide neurosurgeons in placing a brain-computer interface array over the optimal region of the motor cortex.

## ğŸ¯ The Challenge

Design and build a web application that:
- Processes a live stream of neural data from a 1024-channel micro-ECoG array
- Identifies **areas** of functionally tuned neural activity (not just individual transient hotspots)
- Visualizes tuned regions relative to the array position
- Provides clear, intuitive visual guidance for array placement optimization
- Works in the high-pressure environment of an operating room

> **Key insight**: Neural activity over tuned regions is not uniform â€” it depends on cursor movement direction. A good solution identifies coherent **areas of interest** rather than chasing individual activation spikes.

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| **[Overview](docs/overview.md)** | Challenge description, requirements, judging criteria |
| **[Installation](docs/installation.md)** | Setup instructions |
| **[Getting Started](docs/getting_started.md)** | Development workflow and signal processing hints |
| **[Data](docs/data.md)** | Dataset formats, signal content, and processing guidance |
| **[Data Stream](docs/data_stream.md)** | WebSocket protocol reference |
| **[User Persona](docs/user_persona.md)** | Understanding your target user |
| **[Submissions](docs/submissions.md)** | Live evaluation and how to submit |
| **[FAQ](docs/faq.md)** | Common questions and rules |

## ğŸš€ Quick Start

```bash
# 1. Install UV (if needed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Setup
make install

# 3. Download data (start with super_easy, develop with hard)
uv run python -m scripts.download super_easy
uv run python -m scripts.download hard

# 4. Stream data (Terminal 1)
#doesn't work - 
uv run brainstorm-stream --from-file data/hard/
#works -
uv run python scripts/stream_data.py --from-file data/easy/

# 5. Run example app (Terminal 2)
uv run brainstorm-serve
#works -
uv run python scripts/serve.py
# Open http://localhost:8000
```

The example app shows a basic heatmap. **Your solution should go far beyond this!**

See [Installation](docs/installation.md) for detailed setup and [Getting Started](docs/getting_started.md) for development guidance.

## ğŸ“Š The Data

Four difficulty levels on [HuggingFace](https://huggingface.co/datasets/PrecisionNeuroscience/BrainStorm2026-track2):

| Difficulty | Description | Use Case |
|------------|-------------|----------|
| `super_easy` | Crystal-clear signals | Understanding the signal |
| `easy` | Clean signals, minimal noise | Initial development |
| `medium` | Moderate noise | Testing robustness |
| `hard` | Challenging conditions | **Final testing & live evaluation** |

- **Array**: 1024 channels (32Ã—32 grid)
- **Sampling Rate**: 500 Hz
- **Protocol**: WebSocket (JSON batches)

See [Data](docs/data.md) for detailed format and signal processing guidance.

## ğŸ¨ What Makes a Great Solution?

### User Experience (40%)
- Instantly interpretable (< 1 second to understand)
- Readable from 6 feet away (high contrast, large indicators)
- Visualizes tuned **areas** relative to the array
- Provides directional guidance for movement optimization
- Unambiguous "found it" signal when positioned correctly

### Technical Execution (40%)
- Accurate identification of tuned regions
- Real-time performance (low latency, smooth updates)
- Robust to noise and bad channels
- Aggregates signal over time to identify stable areas

### Innovation (20%)
- Novel visualization approaches beyond simple heatmaps
- Creative signal processing
- Compelling video demonstration

## ğŸ› ï¸ What You Can Modify

âœ… **You CAN**:
- Modify or replace the example app completely
- Build a custom backend/middleware (Python, Node, etc.)
- Use any signal processing or visualization approach
- Add any dependencies or frameworks

âŒ **You CANNOT**:
- Modify the data streaming protocol
- Change how data is transmitted during evaluation

## ğŸ“¦ Key Files

```
brainstorm2026-track2/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download.py       # Download datasets from HuggingFace
â”‚   â”œâ”€â”€ stream_data.py    # Stream data locally
â”‚   â”œâ”€â”€ serve.py          # Static file server for example app
â”‚   â””â”€â”€ control_client.py # Send keyboard controls (live eval)
â”œâ”€â”€ example_app/          # Minimal reference implementation
â”œâ”€â”€ data/                 # Downloaded datasets (gitignored)
â””â”€â”€ docs/                 # Full documentation
```

## ğŸ¥ Deliverables

> **âš ï¸ Your submission is a VIDEO** â€” recorded during live evaluation and uploaded to YouTube.

1. **Video demo** (3-5 minutes) â€” Screen recording during live evaluation with voice narration
2. **SUBMISSION.YAML** â€” Updated with your YouTube link and pushed to `main`
3. **Your application** (code repository with documentation)

See [Submissions](docs/submissions.md) for detailed instructions.

## ğŸ¥ Design for the Operating Room

- **User**: Clinical operator with neuroscience PhD (see [User Persona](docs/user_persona.md))
- **Environment**: Crowded, high-stress operating room
- **Viewing Distance**: Up to 6 feet from screen
- **Cognitive Load**: Must be immediately interpretable
- **Stakes**: This guides permanent array placement in a patient's brain

## ğŸ’¡ Tips

- Start simple and iterate
- Test with streaming data early
- **Develop with the `hard` dataset** â€” this matches final evaluation
- Prioritize clarity over complexity
- Think like a surgeon, not a researcher
- Make it readable from 6 feet away
